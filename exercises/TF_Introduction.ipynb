{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import (print_function, division, absolute_import)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Tensorflow \n",
    "\n",
    "## Preface\n",
    "Data engineering makes up for most of the hard work around machine learning. Although we're not attempting to do the data scientists' jobs here, we still need to understand what they're doing to be able to support them best. That's why we'll touch machine learning concepts here. Deliberately, I'll be using *gradient descent* without explaining it. You'll hear about that in later lectures on the subject. What matters here is  that it teaches us about the advanced capabilities available in the chosen technology stack and it provides us a use case that's interesting enough to experience the fun in data engineering.\n",
    "\n",
    "In part II, I'll be getting into some *feature engineering*, which lives in the border area between machine learning and data engineering.\n",
    "\n",
    "# Part I: A toy problem\n",
    "![Baking Powder Machine](../images/baking-powder-packing-machine-500x500.jpg)\n",
    "\n",
    "Let's assume we are responsible for a machine that produces a particular powder ingredient for baking. And we found it little surprising to hear that the quality of the output depends on the humidity within the last inches of the output pipe. Indeed, at certain humidity levels the probability of the product clumping together or sticking to the pipe increased. It's our job to find out what to do. \n",
    "\n",
    "The machine we're talking about has two technical and somewhat mysterious parameters $\\beta_1$ and $\\beta_2$ that can be measured and tuned. We suspect that the humidity is somehow influenced by these parameters and you are supposed to prove it now. \n",
    "\n",
    "A reasonable first hypothesis could be written in mathematical terms as:\n",
    "\n",
    "$$\n",
    "h= A_1 \\cdot \\beta_1 + A_2 \\cdot \\beta_2 + C\n",
    "$$\n",
    "\n",
    "where $h$ is the measured humidity, and $A_1, A_2$ and $C$ are three model parameters that we need to compute now. Any non-zero value for $A_1, A_2$ will confirm our suspicion.\n",
    "\n",
    "Here's how we take some measurements. Note that we also recorded the weekday and the hour of the day when the measurement took place. At this point in time we consider them irrelevant. In the second part of this exercise though, we'll see that they can actually be meaningfully included in the analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data first\n",
    "\n",
    "```measure(N) ``` creates a [pandas](https://pandas.pydata.org/) data frame with $N$ measurement records in it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from measurements import measure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = measure(5)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = measure(10000)\n",
    "data.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First Evidence for correlation\n",
    "Scatter plots are easily created with pandas. We can see that $\\beta_1$ has a strong correlation with the humidity and it also exhibits a particularly unusual distribution, as the field has a sharper lower edge. We'll be investigating this in part II of this exercise. With $\\beta_2$, there might indeed be a very weak negative correlation, but we can't be certain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = measure(200)\n",
    "data.plot.scatter(x='beta1', y='humidity')\n",
    "data.plot.scatter(x='beta2', y='humidity');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Working with tensors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In Tensorflow, everything is a tensor. A number is a rank-0 tensor - sometimes code-named *scalar*, A vector is a rank-1 tensor, a matrix is a rank-2 tensor and all higher-ranked tensors are just rank-n tensors.\n",
    "\n",
    "Now, we'll make the hypothesis that the correlations can be described by an *affine* function of those technical parameters $\\beta_1$ and $\\beta_2$. We simply encode this hypothesis using matrix multiplication:\n",
    "\n",
    "$$\n",
    "h(\\beta_1, \\beta_2) = (A_1, A_2) \\cdot\n",
    "\\left( \n",
    "\\begin {array} {c}\n",
    "\\beta_1 \\\\\n",
    "\\beta_2\n",
    "\\end{array}\n",
    "\\right) + C\n",
    "$$\n",
    "\n",
    "which is equivalent to saying:\n",
    "\n",
    "$$\n",
    "h(\\beta_1, \\beta_2) = A_1 \\beta_1 +  A_2 \\beta_2 + C\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now all that's left is trying to find $A_1, A_2$ and $C$ such that the above function $h$ best reproduces the actually performed measurements for the humidity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```tf.Variable```s are those tensors that are meant to be tuned during an optimization process. Since we want to tweak and tune $A_1, A_2$ and $C$, we create variables for those."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A 1x2 tensor (matrix) for the coefficients A_i\n",
    "# We're starting with arbitrary values\n",
    "A = tf.Variable([[1., 2.]], name=\"coefficients_A\", dtype=tf.float32) \n",
    "A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A scalar tensor for the *bias* C\n",
    "C = tf.Variable(3., name=\"bias_C\", dtype=tf.float32)\n",
    "C"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can actually look into those tensors and inspect their values with the help of a session (just ignore the initializer for the time being):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.Session() as session:\n",
    "    session.run(tf.global_variables_initializer())\n",
    "    print(C.eval())\n",
    "    print(A.eval())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use ```tf.placeholder```s for the input data and the *labels*. In machine learning, *labels* represent the *true* values. When *labels* are provided, we say that we're doing *supervised learning*. Here, the labels are the true (measured) humidity. And that's what we want the hypothesis to mimic (reproduce). The ```None``` dimension is left open intentionally as it is this dimension that accounts for the number of input records that will be processed at once."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "beta = tf.placeholder(shape=(2,None), name=\"beta\", dtype=tf.float32) \n",
    "lbls = tf.placeholder(shape=(1,None), name=\"true_humidity\", dtype=tf.float32)  \n",
    "beta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The hypothesis function as a computational graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The linear humidity hypothesis ```h``` represents a computational graph that we can actually navigate to see its branches and leaves like demonstrated below. \n",
    "\n",
    "You'll read the plus operator in the following expression\n",
    "```\n",
    "h = tf.matmul(A, beta) + C\n",
    "```\n",
    "as: take the tensor to the left, attach it to the left of a \"+\" node, then take the tensor to the right (C) and attach it to the right side of that node. Return the \"+\" node and with it the attached computational graph."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "h=\n",
    "         +\n",
    "      /    \\ \n",
    " matmul      C\n",
    " /    \\\n",
    "A     beta\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h = tf.matmul(A, beta) + C\n",
    "# h represents the + operation: the root of the tree\n",
    "print(h)\n",
    "print()\n",
    "\n",
    "# The matmul operation is the first input of the + operator\n",
    "print(h.op.inputs[0])\n",
    "print()\n",
    "\n",
    "# The coefficients A to the left of the matmul operation:\n",
    "print(h.op.inputs[0].op.inputs[0])\n",
    "print()\n",
    "\n",
    "# The input beta to the left of the matmul operation:\n",
    "print(h.op.inputs[0].op.inputs[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluating the graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use a Tensorflow session object to evaluate the computational graph at the particular values given by ```beta_input```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "beta_input = [list(data['beta1']), list(data['beta2'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "beta_input[0][:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Executing the hypothesis\n",
    "It's always the same: Initialize all variables and then run the session with the desired tensor node (or a list of tensors). The graph will then resolve the dependencies transitively, compute those and work back to the root where the final result is then computed and passed as a regular number or numpy array. Resolving the placeholders required us to provide some input, which we did using the so-called ```feed_dict```. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "init = tf.global_variables_initializer()\n",
    "with tf.Session() as session:\n",
    "    session.run(init)\n",
    "    model_humidity = session.run(h, feed_dict={beta: beta_input })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "true_humidity = list(data['humidity'])\n",
    "true_humidity[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zip(true_humidity, list(model_humidity.squeeze()))[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results differ vastly from the true humidity - unsurprisingly so - we haven't tuned the parameters $A$ and $C$ yet.  \n",
    "\n",
    "#### Delegating execution specifics to the session\n",
    "Note that the complexity seen above makes sense because once the graph is constructed it's up to the Tensorflow runtime to find out how to compute the results in the most efficient way, taking also the available advanced system resources, such as GPUs or TPUs into account. You as a developer need not bother, just pass your graph to the session.\n",
    "\n",
    "#### All input in one go\n",
    "Note also that all the records will processed in a single go. There is no loop over the input data. Indeed all the input data forms a single $N \\times 2$ matrix with $N$ being the number of samples. That makes computation truly efficient, as GPUs and TPUs can perform thousands of mathematical operations in parallel, and there's no forth and back between the Python script interpreter and the underlying computational infrastructure."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Error = Distance between model and reality"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want the model humidity to be as close as possible to the *true* humidity. To achieve that we need to measure how close the model has come to the true humidity. The mean squared error is a good candidate for that purpose, and we'll use the placeholder ```lbls``` ('labels' is commonly used in machine learning) that we introduced previously to represent the true humidity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "msq_error = tf.losses.mean_squared_error(h,lbls)\n",
    "\n",
    "# We need to fit the shape to the hypothesis input placeholder\n",
    "true_humidity = [true_humidity] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.Session() as session:\n",
    "    session.run(init)\n",
    "    error = session.run(msq_error, \n",
    "                        feed_dict={\n",
    "                            beta: beta_input, \n",
    "                            lbls: true_humidity })\n",
    "print(error)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tuning the parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Note: We'll use some advanced concepts that we can't cover to the full extent here. Please refer to any of the quazillions of amazing introductory sources available on the internet to get familiar with the concept of gradient descent. [This video by 3Blue1Brown talks about gradient descent in the concept of neural networks, but still it's super-easy to grasp because of its amazing visualizations](https://www.youtube.com/watch?v=IHZwWFHWa-w&t=1s)*.\n",
    "\n",
    "The *gradient* is a measure of how much the value of a function changes, when its input changes infinitesimaly. If e.g. the gradient of the distance with respect to $A_1$ is positive then that means making $A_1$ a bit smaller would also make the distance smaller. And that's exactly what we're trying to achieve. So what we'll do is iteratively substract a fraction (defined by some small learning rate epsilon $\\varepsilon$) of the gradient from the values of $A$ and $B$. And we do this in vector form."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epsilon = 1e-2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grad_A = tf.gradients(msq_error, A)\n",
    "grad_C = tf.gradients(msq_error, C)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note how easy it is to calculate gradients here. It's all built into the framework. That's super-easy here, because the computational graph allows the computation of any derivative from basic principles according to the rules that we all (should've) learned in our calculus classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Tune_A = tf.assign_add( A, tf.multiply(grad_A[0], -epsilon))\n",
    "Tune_C = tf.assign_add( C, tf.multiply(grad_C[0], -epsilon))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In mathematical terms, what we do is:\n",
    "$$\n",
    "A \\leftarrow A - \\varepsilon \\cdot \\frac{\\partial}{\\partial A} \\text{MSE}\n",
    "$$\n",
    "\n",
    "$$\n",
    "C \\leftarrow C - \\varepsilon \\cdot \\frac{\\partial}{\\partial C} \\text{MSE}\n",
    "$$\n",
    "\n",
    "with MSE being the mean squared error ```msq_error```. Computationally, evaluating ```Tune_A``` will have the *side effect* of changing the value of variable ```A``` and ```Tune_C``` will do that with ```C```."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We create some independent test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = measure(20)\n",
    "test_true = [list(test_data['humidity'])]\n",
    "test_beta = [list(test_data['beta1']), list(test_data['beta2'])]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "...and the data that we use for the optimization process:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = measure(10000)\n",
    "true_humidity = [list(data['humidity'])]\n",
    "beta_input = [list(data['beta1']), list(data['beta2'])]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the below implementation is deliberately simple and a little inefficient to keep things more readable. The code cell below computes the gradients and adjusts the parameters 200 times and spits out a list of all values for ```msq_error```. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "errors = []\n",
    "with tf.Session() as session:\n",
    "    session.run(init)\n",
    "    for count in range(200):\n",
    "        \n",
    "        # compute the current error/distance\n",
    "        error = session.run(msq_error, feed_dict={\n",
    "            beta: beta_input, \n",
    "            lbls: true_humidity \n",
    "        })\n",
    "        errors.append(error)\n",
    "        \n",
    "        # tune and tweak the parameters a little\n",
    "        session.run([Tune_A, Tune_C], feed_dict={\n",
    "            beta: beta_input, \n",
    "            lbls: true_humidity \n",
    "        })\n",
    "\n",
    "    # Storing test results and parameters\n",
    "    test_results, a, c = session.run([h, A, C], feed_dict={\n",
    "        beta: test_beta})\n",
    "        \n",
    "print(errors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The steadily decreasing list of errors above is a very welcome sign. It means: Our model's output values are indeed getting closer and closer to the *true* humidity. I dare say our model is successfully converging to the *truth*. And we should definitely see that when we explicitly compare true humidity with the one that our model *predicts*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zip(list(test_results[0]), test_true[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "The parameters $A_1, A_2$ and $C$ have eventually converged to:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(a[0]), c"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That concludes the first part of this exercise. You learned to construct and execute computational graphs with Tensorflow and apply one of the most fundamental techniques, namely gradient descent to solve an analytical problem. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Part II: Feature Engineering\n",
    "\n",
    "So, are we happy? To a large degree we could indeed be. We have found convincing evidence that those\n",
    "mysterious parameters indeed have some influence on the humidity, so tuning them appropriately may help overcome the afforementioned problems. Yet, we began to suspect that something else is wrong. To provide some more evidence, let's have a look at the distribution of the errors over a larger data set.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following context, we'll be working with the parameters $A_1, A_2, B$ that we've found previously and thus make them constants. For $10000$ measurements we record how far we have predicted from the measured humidity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = measure(10000)\n",
    "true_humidity = [list(data['humidity'])]\n",
    "beta_input = [list(data['beta1']), list(data['beta2'])]\n",
    "\n",
    "A = tf.constant([2, -.5], shape=(1,2), dtype=tf.float32)\n",
    "C = tf.constant(18.5, shape=(), dtype=tf.float32)\n",
    "h = tf.matmul(A, beta) + C\n",
    "error = (h-lbls)\n",
    "with tf.Session() as session:\n",
    "    errors = session.run(\n",
    "        error, feed_dict={beta: beta_input, lbls: true_humidity})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(errors[0]).hist(bins=30);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What a surprise! Error distributions of that kind indicate that the linear model is overlooking something. It appears ok for the majority of measurements (the big peak around 0), but a small subset of measurements is apparently influenced by still unknown forces. \n",
    "\n",
    "Now, how to approach that problem? At the moment, we're only considering $\\beta_1$ and $\\beta_2$ as influence factors. Maybe there are other factors. Maybe on certain days or at certain times of the day the humidity is influenced by other events?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "sns.set()\n",
    "data = measure(2000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```seaborn``` introduces a convenience layer on top of matplotlib using pandas under the hood. It's extremely helpful for quick explorations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.catplot(x='weekday', y='humidity', kind='bar', data=data);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Look at the average humidity for each day of the week. Still hard to say. Thursdays and Sundays appear particularly low. But do we have sufficient statistics to conclude? Let's have a look at the hours of the day:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.catplot(x='hour', y='humidity', kind='bar', data=data);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The error bars are not big enough to dismiss the differences as statistical noise. So we're getting closer. But still it's not easy to tell what's really going on. \n",
    "\n",
    "*(Hint: Don't be shy to look at the statistics of 50'000 data points)*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Well, here we are. It's obvious that we need to take weekdays and hours of day into account. But that's not so trivial. While $\\beta_1$ and $\\beta_2$ were suspected to influence the humidity somewhat linearly, that is certainly not true for hour and weekday. Data engineering has the answer: *one-hot* encoding. That's almost always a successful strategy, if it is somehow a *yes-or-no* or a *one-in-N* aspect that governs the problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "#### One-Hot encoding weekdays"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use a neat combination of Python tricks to one-hot encode a number: The $n$-th column (or row) vector of the identity matrix (```np.eye``` in the numpy world), is the one-hot encoding for $n$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ID = np.eye(7)\n",
    "ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for n in [0, 3, 6]:\n",
    "    print(\"%s -> %s\" % (n, ID[n]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You see the $1$s at the position given by the index. So, Monday would be represented by the first vector, Thu by the second and Sun by the third.\n",
    "\n",
    "Here comes the even cooler trick: We can use an array of indices rather than single indices one by one:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indices = [0,3,6]\n",
    "ID[indices]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See: All three days one-hot-encoded in a single go"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "#### Preparing data for the next round\n",
    "We'll need a full dataset with one-hot encoded weekdays."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = measure(10000)\n",
    "data[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This does one hot encoding and gets the shape right.\n",
    "def one_hot(df, size):\n",
    "    ords = list(df)\n",
    "    return list(np.transpose(np.eye(size)[ords]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Putting the one-hot encoded values in a data frame. That's not necessary, but seeing it this way makes it much clearer what one-hot encoding actually means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "days_one_hot = one_hot(data['weekday'], 7)\n",
    "weekdays=['Mon', 'Tue', 'Wed', 'Thu', 'Fri', 'Sat', 'Sun']\n",
    "for i in range(7):\n",
    "    data[weekdays[i]]=days_one_hot[i]\n",
    "data[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have $2+7=9$ different input *signals*, and we'll use a $1 \\times 9$ parameter matrix to deal with them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_input(data):\n",
    "    days_one_hot = one_hot(data['weekday'], 7)\n",
    "    weekdays=['Mon', 'Tue', 'Wed', 'Thu', 'Fri', 'Sat', 'Sun']\n",
    "    for i in range(7):\n",
    "        data[weekdays[i]]=days_one_hot[i]\n",
    "    all_input = [\n",
    "        list(data['beta1']), \n",
    "        list(data['beta2']),\n",
    "        list(data['Mon']),\n",
    "        list(data['Tue']),\n",
    "        list(data['Wed']),\n",
    "        list(data['Thu']),\n",
    "        list(data['Fri']),\n",
    "        list(data['Sat']),\n",
    "        list(data['Sun'])\n",
    "    ]\n",
    "    return all_input\n",
    "\n",
    "true_humidity = [list(data['humidity'])]\n",
    "all_input = create_input(data)\n",
    "\n",
    "test_data = measure(10000)\n",
    "test_input = create_input(test_data)\n",
    "test_humidity = [list(test_data['humidity'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Exercise\n",
    "Write the code for the new hypothesis function like below:\n",
    "\n",
    "\n",
    "$$\n",
    "h(...) = (A_1, A_2, A_m, A_t, A_w, A_{th}, A_f, A_s, A_{su}) \\cdot\n",
    "\\left( \n",
    "\\begin {array} {c}\n",
    "\\beta_1 \\\\\n",
    "\\beta_2 \\\\\n",
    "\\text{Mon} \\\\\n",
    "\\text{Tue} \\\\\n",
    "\\text{Wed} \\\\\n",
    "\\text{Thu} \\\\\n",
    "\\text{Fri} \\\\\n",
    "\\text{Sat} \\\\\n",
    "\\text{Sun}\n",
    "\\end{array}\n",
    "\\right) + C\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#    Your turn: Replace the None placeholder with meaningful code.\n",
    "#\n",
    "# A 1x9 tensor (matrix) for the coefficients A_i\n",
    "# Start with arbitrary (but small) values\n",
    "A = None \n",
    "A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#    Nothing new here\n",
    "#\n",
    "# A scalar tensor for the *bias* C\n",
    "C = None\n",
    "C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "#  Placeholders shape=(9, None) for input data, (1, None) for labels\n",
    "#\n",
    "input_placeholder = None \n",
    "labels_placeholder = None\n",
    "input_placeholder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#    Thanks to the vector notation, the hypothesis function doesn't \n",
    "#    change\n",
    "#\n",
    "h = None\n",
    "h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "#   Mean squared error: Nothing changes here\n",
    "#\n",
    "msq_error = None\n",
    "msq_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "#   gradients and updating: Nothing changes here\n",
    "#\n",
    "grad_A = None\n",
    "grad_C = None\n",
    "Tune_A = None\n",
    "Tune_C = None\n",
    "Tune_A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "errors = []\n",
    "with tf.Session() as session:\n",
    "    session.run(tf.global_variables_initializer())\n",
    "    for count in range(200):\n",
    "        \n",
    "        # compute the current error/distance\n",
    "        error = session.run(msq_error, feed_dict={\n",
    "            input_placeholder: all_input, \n",
    "            labels_placeholder: true_humidity \n",
    "        })\n",
    "        errors.append(error)\n",
    "        \n",
    "        # tune and tweak the parameters a little\n",
    "        session.run([Tune_A, Tune_C], feed_dict={\n",
    "            input_placeholder: all_input, \n",
    "            labels_placeholder: true_humidity \n",
    "        })\n",
    "\n",
    "    # Storing test results and parameters\n",
    "    test_results, a, c = session.run([h, A, C], feed_dict={\n",
    "        input_placeholder: test_input})\n",
    "        \n",
    "print(errors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a, c"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Look at the parameters $A_{Thu}$ (6th) and $A_{Sun}$ (9th). These are lower than the other weekdays. The model found out that the humidity is slightly lower on these days. That's a little bit of an achievement. Although, when we look at the error distribution like we did before:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = tf.constant(a, shape=(1,9), dtype=tf.float32)\n",
    "C = tf.constant(c, shape=(), dtype=tf.float32)\n",
    "h = tf.matmul(A, input_placeholder) + C\n",
    "error = (h-labels_placeholder)\n",
    "with tf.Session() as session:\n",
    "    errors = session.run(\n",
    "        error, feed_dict={\n",
    "            input_placeholder: test_input, \n",
    "            labels_placeholder: test_humidity})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(errors[0]).hist(bins=30);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the weekday together with the $\\beta$ parameters still don't explain the humidity observations sufficiently. As a matter of fact, if you also hot-encode the hours of the day and perform a feature crossing to produce a 168-dimensional hour-of-the-week feature, then you'll see the bump disappear and you'll find that the model *learns to understand* that there's additional humidity at certain hours of the week. Give it a try!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#    Your turn\n",
    "#\n",
    "# A 1x9 tensor (matrix) for the coefficients A_i\n",
    "# Start with arbitrary (but small) values\n",
    "A = tf.Variable([[1,1,0,0,0,0,0,0,0]], \n",
    "                name=\"coefficients_A\", dtype=tf.float32) \n",
    "A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#    Nothing new here\n",
    "#\n",
    "# A scalar tensor for the *bias* C\n",
    "C = tf.Variable(3., name=\"bias_C\", dtype=tf.float32)\n",
    "C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "#  Placeholders shape=(9, None) for input data, (1, None) for labels\n",
    "#\n",
    "input_placeholder = tf.placeholder(\n",
    "    shape=(9,None), name=\"all_input\", dtype=tf.float32) \n",
    "labels_placeholder = tf.placeholder(\n",
    "    shape=(1,None), name=\"true_humidity\", dtype=tf.float32)  \n",
    "input_placeholder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#    Thanks to the vector notation, the hypothesis function doesn't \n",
    "#    change\n",
    "#\n",
    "h = tf.matmul(A, input_placeholder) + C\n",
    "h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "#   Mean squared error: Nothing changes here\n",
    "#\n",
    "msq_error = tf.losses.mean_squared_error(h,labels_placeholder)\n",
    "msq_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "#   gradients and updating: Nothing changes here\n",
    "#\n",
    "grad_A = tf.gradients(msq_error, A)\n",
    "grad_C = tf.gradients(msq_error, C)\n",
    "Tune_A = tf.assign_add( A, tf.multiply(grad_A[0], -epsilon))\n",
    "Tune_C = tf.assign_add( C, tf.multiply(grad_C[0], -epsilon))\n",
    "Tune_A"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
