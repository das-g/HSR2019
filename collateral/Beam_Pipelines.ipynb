{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.13.1'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import tempfile\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import tensorflow_transform as tft\n",
    "from tensorflow_transform.tf_metadata import dataset_schema\n",
    "from tensorflow_transform.tf_metadata import dataset_metadata\n",
    "import apache_beam as beam\n",
    "import tensorflow_transform.beam as tft_beam\n",
    "from tensorflow_transform.beam.tft_beam_io import transform_fn_io\n",
    "from apache_beam.options.pipeline_options import PipelineOptions\n",
    "import tensorflow_transform.beam.impl as beam_impl\n",
    " \n",
    "from measurements import measure\n",
    "\n",
    "tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You can find the files at:\n",
      "/tmp/tmpsrnhj_zd/signature.csv /tmp/tmpsrnhj_zd/training.csv /tmp/tmpsrnhj_zd/training.tfr\n"
     ]
    }
   ],
   "source": [
    "temp_dir = tempfile.mkdtemp()\n",
    "\n",
    "# Signature data: Just the way it'll arrive at prediction time\n",
    "signature_csv = os.path.join(temp_dir, \"signature.csv\")\n",
    "\n",
    "# Training data: maybe scaled or further pre-processed.\n",
    "training_csv = os.path.join(temp_dir, \"training.csv\")\n",
    "\n",
    "# TFRecord: Allows for high performance input into computational graphs\n",
    "training_tfr = os.path.join(temp_dir, \"training.tfr\")\n",
    "\n",
    "print(\"You can find the files at:\")\n",
    "print(signature_csv, training_csv, training_tfr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm -rf temp_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>beta1</th>\n",
       "      <th>beta2</th>\n",
       "      <th>hour</th>\n",
       "      <th>humidity</th>\n",
       "      <th>weekday</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-2.536744</td>\n",
       "      <td>-0.348205</td>\n",
       "      <td>22</td>\n",
       "      <td>12.726654</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2.741431</td>\n",
       "      <td>-1.288184</td>\n",
       "      <td>10</td>\n",
       "      <td>24.225378</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4.916265</td>\n",
       "      <td>4.722238</td>\n",
       "      <td>1</td>\n",
       "      <td>27.175469</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-3.174294</td>\n",
       "      <td>-4.913538</td>\n",
       "      <td>0</td>\n",
       "      <td>14.737376</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-2.951847</td>\n",
       "      <td>-2.143401</td>\n",
       "      <td>9</td>\n",
       "      <td>12.510398</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      beta1     beta2  hour   humidity  weekday\n",
       "0 -2.536744 -0.348205    22  12.726654        1\n",
       "1  2.741431 -1.288184    10  24.225378        2\n",
       "2  4.916265  4.722238     1  27.175469        2\n",
       "3 -3.174294 -4.913538     0  14.737376        1\n",
       "4 -2.951847 -2.143401     9  12.510398        4"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = measure(5)\n",
    "data.to_csv(signature_csv, index=None)\n",
    "data = pd.read_csv(signature_csv)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Specify the input and output formats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "ORDERED_SIGNATURE_COLUMNS=[\"beta1\", \"beta2\", \"hour\", \"humidity\", \"weekday\"]\n",
    "header = bytes(\",\".join(ORDERED_SIGNATURE_COLUMNS), 'UTF-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_spec = {\n",
    "    'beta1': tf.io.FixedLenFeature([1], tf.float32),\n",
    "    'beta2': tf.io.FixedLenFeature([1], tf.float32),\n",
    "    'weekday': tf.io.FixedLenFeature([1], tf.int64),\n",
    "    'hour': tf.io.FixedLenFeature([1], tf.int64),\n",
    "    'humidity': tf.io.FixedLenFeature([1], tf.float32)\n",
    "}\n",
    "schema = dataset_schema.from_feature_spec(feature_spec)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create an encoder and test it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'weekday': array([4]), 'humidity': array([1.234], dtype=float32), 'beta1': array([10.201], dtype=float32), 'beta2': array([10.101], dtype=float32), 'hour': array([3])}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "b'10.201,10.101,3,1.234,4'"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "csv_encoder = tft.coders.CsvCoder(ORDERED_SIGNATURE_COLUMNS, schema)\n",
    "records = csv_encoder.decode(\"10.201, 10.101, 3,1.234,4\")\n",
    "print(records)\n",
    "csv_encoder.encode(records)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Apache Beam pipeline "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_data(row):\n",
    "    print(row)\n",
    "    return row"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dry run - Everything working?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'weekday': array([1]), 'humidity': array([12.726654], dtype=float32), 'beta1': array([-2.5367444], dtype=float32), 'beta2': array([-0.34820482], dtype=float32), 'hour': array([22])}\n",
      "{'weekday': array([2]), 'humidity': array([24.225378], dtype=float32), 'beta1': array([2.7414305], dtype=float32), 'beta2': array([-1.2881843], dtype=float32), 'hour': array([10])}\n",
      "{'weekday': array([2]), 'humidity': array([27.175468], dtype=float32), 'beta1': array([4.9162645], dtype=float32), 'beta2': array([4.7222385], dtype=float32), 'hour': array([1])}\n",
      "{'weekday': array([1]), 'humidity': array([14.737376], dtype=float32), 'beta1': array([-3.1742942], dtype=float32), 'beta2': array([-4.9135385], dtype=float32), 'hour': array([0])}\n",
      "{'weekday': array([4]), 'humidity': array([12.510398], dtype=float32), 'beta1': array([-2.9518473], dtype=float32), 'beta2': array([-2.1434014], dtype=float32), 'hour': array([9])}\n"
     ]
    }
   ],
   "source": [
    "with beam.Pipeline('DirectRunner', PipelineOptions()) as p:\n",
    "\n",
    "    csv_encoder = tft.coders.CsvCoder(ORDERED_SIGNATURE_COLUMNS, schema)    \n",
    "\n",
    "    _ = (p \n",
    "         | 'read_from_csv' >> beam.io.ReadFromText(\n",
    "             file_pattern=signature_csv, coder=csv_encoder, skip_header_lines=1)\n",
    "         \n",
    "         | 'process_records' >> beam.Map(process_data)\n",
    "         \n",
    "         | 'write_to_csv' >> beam.io.WriteToText(\n",
    "             file_path_prefix=training_csv, coder=csv_encoder, header=header)\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading from:  /tmp/tmpsrnhj_zd/training.csv-00000-of-00001\n",
      "beta1,beta2,hour,humidity,weekday\n",
      "-2.5367444,-0.34820482,22,12.726654,1\n",
      "2.7414305,-1.2881843,10,24.225378,2\n",
      "4.9162645,4.7222385,1,27.175468,2\n",
      "-3.1742942,-4.9135385,0,14.737376,1\n",
      "-2.9518473,-2.1434014,9,12.510398,4\n"
     ]
    }
   ],
   "source": [
    "!echo \"Reading from: \" $training_csv*\n",
    "!cat $training_csv*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Serious transformation: Scale $\\beta_1$ and $\\beta_2$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = measure(20000)\n",
    "data.to_csv(signature_csv, index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_data(row):\n",
    "    for c in ['beta1', 'beta2']:\n",
    "        row[c] = tft.scale_to_0_1(row[c])\n",
    "    return row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "signature_metadata = dataset_metadata.DatasetMetadata(schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets added to graph.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets added to graph.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:No assets to write.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:No assets to write.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:SavedModel written to: /tmp/tmpsrnhj_zd/tftransform_tmp/287ff8e2554a4e3bb62b838b363c886a/saved_model.pb\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:SavedModel written to: /tmp/tmpsrnhj_zd/tftransform_tmp/287ff8e2554a4e3bb62b838b363c886a/saved_model.pb\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets added to graph.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets added to graph.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:No assets to write.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:No assets to write.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:SavedModel written to: /tmp/tmpsrnhj_zd/tftransform_tmp/58bce881fccb441e8d175e65eb1a4c43/saved_model.pb\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:SavedModel written to: /tmp/tmpsrnhj_zd/tftransform_tmp/58bce881fccb441e8d175e65eb1a4c43/saved_model.pb\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets added to graph.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets added to graph.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:No assets to write.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:No assets to write.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:SavedModel written to: /tmp/tmpsrnhj_zd/tftransform_tmp/d9aab3ef32dd4d07829f6b92a26faf21/saved_model.pb\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:SavedModel written to: /tmp/tmpsrnhj_zd/tftransform_tmp/d9aab3ef32dd4d07829f6b92a26faf21/saved_model.pb\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n",
      "WARNING:root:Deleting 1 existing files in target path matching: -*-of-%(num_shards)05d\n"
     ]
    }
   ],
   "source": [
    "csv_encoder = tft.coders.CsvCoder(ORDERED_SIGNATURE_COLUMNS, schema)    \n",
    "tfr_encoder = tft.coders.ExampleProtoCoder(schema)            \n",
    "\n",
    "metadata_dir = os.path.join(temp_dir, \"metadata\")\n",
    "with beam.Pipeline('DirectRunner', PipelineOptions()) as p:\n",
    "\n",
    "    #\n",
    "    # The context is provided for the AnalyseAndTransform step.\n",
    "    # That step needs a hand to do its magic.\n",
    "    #\n",
    "    with tft_beam.Context(temp_dir=temp_dir):\n",
    "\n",
    "        #\n",
    "        # Read from csv, skip headers. Note that we use ordered columns in the encoder\n",
    "        #\n",
    "        signature_data = ( p | 'read_from_csv' \n",
    "            >> beam.io.ReadFromText(\n",
    "                 file_pattern=signature_csv, coder=csv_encoder, skip_header_lines=1))\n",
    "\n",
    "        #\n",
    "        # attach the metadata: required for AnalyzeAndTransform\n",
    "        #\n",
    "        signature_data = ( signature_data, signature_metadata)\n",
    "\n",
    "        #\n",
    "        # Do the magic two steps and return also the transform-function\n",
    "        #\n",
    "        data_and_metadata, transform_fn = ( signature_data | \"AnalyzeAndTransform\" \n",
    "                         >> beam_impl.AnalyzeAndTransformDataset(process_data))\n",
    "        \n",
    "        #\n",
    "        # split data and metadata\n",
    "        #\n",
    "        training_data, training_metadata = data_and_metadata\n",
    "\n",
    "        #\n",
    "        # Write the resulting data to a csv file\n",
    "        #\n",
    "        _ = (training_data | 'write_to_csv' \n",
    "             >> beam.io.WriteToText(\n",
    "                 file_path_prefix=training_csv, coder=csv_encoder, header=header))\n",
    "\n",
    "        _ = (training_data | 'write_to_tfr' \n",
    "             >> beam.io.WriteToTFRecord(\n",
    "                 file_path_prefix=training_tfr, coder=tfr_encoder))\n",
    "\n",
    "        \n",
    "        \n",
    "        #\n",
    "        # Eventually, save the transform function for re-use at prediction time.\n",
    "        #\n",
    "        _ = (transform_fn | 'WriteTransformFn' \n",
    "             >> transform_fn_io.WriteTransformFn(metadata_dir))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading from:  /tmp/tmpsrnhj_zd/training.csv-00000-of-00001\n",
      "\n",
      "0.3818975,0.5458483,12,17.193748,2\n",
      "0.16038401,0.5800147,9,10.631513,2\n",
      "0.44184422,0.7403201,17,18.30096,5\n",
      "0.7561718,0.61078686,2,23.344,1\n",
      "0.73403597,0.96532387,4,21.984755,2\n",
      "\n",
      "metadata is here:\n",
      "transformed_metadata  transform_fn\n",
      "\n",
      "TFRecords are here:  /tmp/tmpsrnhj_zd/training.tfr-00000-of-00001\n"
     ]
    }
   ],
   "source": [
    "!echo \"Reading from: \" $training_csv*\n",
    "!echo\n",
    "!cat $training_csv* | tail -5\n",
    "!echo\n",
    "!echo metadata is here:\n",
    "!ls $metadata_dir\n",
    "!echo \n",
    "!echo \"TFRecords are here: \" $training_tfr*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wrap up"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're now able to analyze and process any size of data, scale particular features to the interval $[0, 1]$ while saving the function that actually did it for later. We'll need that function to apply exactly the same scaling to the incoming data at prediction time. \n",
    "Note that by simply swapping ```'DirectRunner'``` in the Apache Beam pipeline by ```'DataFlowRunner'``` in an adequately configured GCP environment, we could have the pipeline executed on an arbitrarily large cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
