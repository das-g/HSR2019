{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/wgiersche/workspace/venv3/lib/python3.5/site-packages/apache_beam/__init__.py:84: UserWarning: Running the Apache Beam SDK on Python 3 is not yet fully supported. You may encounter buggy behavior or missing features.\n",
      "  'Running the Apache Beam SDK on Python 3 is not yet fully supported. '\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'1.13.1'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import tempfile\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import tensorflow_transform as tft\n",
    "from tensorflow_transform.tf_metadata import dataset_schema\n",
    "from tensorflow_transform.tf_metadata import dataset_metadata\n",
    "import apache_beam as beam\n",
    "import tensorflow_transform.beam as tft_beam\n",
    "from tensorflow_transform.beam.tft_beam_io import transform_fn_io\n",
    "from apache_beam.options.pipeline_options import PipelineOptions\n",
    "import tensorflow_transform.beam.impl as beam_impl\n",
    " \n",
    "from measurements import measure\n",
    "\n",
    "tf.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing measurement data for Machine Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A little trick: Take a note of the temporary directory containing the data for re-use in other notebooks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_dir = \"temp_dir = '%s'\" % tempfile.mkdtemp()\n",
    "with open('temp_dir.py', 'w') as file:\n",
    "    file.write(temp_dir)\n",
    "from temp_dir import temp_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'./tmp'"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp_dir = './tmp'\n",
    "temp_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You can find the files at:\n",
      "./tmp/signature_train.csv ./tmp/training.csv ./tmp/training.tfr\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Signature data: Just the way it'll arrive at prediction time\n",
    "signature_csv_train = os.path.join(temp_dir, \"signature_train.csv\")\n",
    "signature_csv_eval = os.path.join(temp_dir, \"signature_eval.csv\")\n",
    "signature_csv_test = os.path.join(temp_dir, \"signature_test.csv\")\n",
    "\n",
    "# Training data: maybe scaled or further pre-processed.\n",
    "training_csv = os.path.join(temp_dir, \"training.csv\")\n",
    "eval_csv = os.path.join(temp_dir, \"eval.csv\")\n",
    "\n",
    "# TFRecord: Allows for high performance input into computational graphs\n",
    "training_tfr = os.path.join(temp_dir, \"training.tfr\")\n",
    "eval_tfr = os.path.join(temp_dir, \"eval.tfr\")\n",
    "\n",
    "print(\"You can find the files at:\")\n",
    "print(signature_csv_train, training_csv, training_tfr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The orginal data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>beta1</th>\n",
       "      <th>beta2</th>\n",
       "      <th>hour</th>\n",
       "      <th>humidity</th>\n",
       "      <th>weekday</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3.601713</td>\n",
       "      <td>4.718712</td>\n",
       "      <td>15</td>\n",
       "      <td>23.255400</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.777682</td>\n",
       "      <td>-4.617800</td>\n",
       "      <td>21</td>\n",
       "      <td>24.903056</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3.559258</td>\n",
       "      <td>-2.819708</td>\n",
       "      <td>6</td>\n",
       "      <td>26.001160</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3.988322</td>\n",
       "      <td>4.907078</td>\n",
       "      <td>23</td>\n",
       "      <td>24.061360</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-2.560512</td>\n",
       "      <td>3.170083</td>\n",
       "      <td>2</td>\n",
       "      <td>14.927021</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      beta1     beta2  hour   humidity  weekday\n",
       "0  3.601713  4.718712    15  23.255400        0\n",
       "1  1.777682 -4.617800    21  24.903056        4\n",
       "2  3.559258 -2.819708     6  26.001160        1\n",
       "3  3.988322  4.907078    23  24.061360        1\n",
       "4 -2.560512  3.170083     2  14.927021        0"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = measure(5)\n",
    "data.to_csv(signature_csv_train, index=None)\n",
    "data = pd.read_csv(signature_csv_train)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Specify the input and output formats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "ORDERED_SIGNATURE_COLUMNS=[\"beta1\", \"beta2\", \"hour\", \"humidity\", \"weekday\"]\n",
    "header = bytes(\",\".join(ORDERED_SIGNATURE_COLUMNS), 'UTF-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_spec = {\n",
    "    'beta1': tf.io.FixedLenFeature([1], tf.float32),\n",
    "    'beta2': tf.io.FixedLenFeature([1], tf.float32),\n",
    "    'weekday': tf.io.FixedLenFeature([1], tf.int64),\n",
    "    'hour': tf.io.FixedLenFeature([1], tf.int64),\n",
    "    'humidity': tf.io.FixedLenFeature([1], tf.float32)\n",
    "}\n",
    "schema = dataset_schema.from_feature_spec(feature_spec)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create an encoder and test it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'humidity': array([1.234], dtype=float32), 'beta1': array([10.201], dtype=float32), 'beta2': array([10.101], dtype=float32), 'weekday': array([4]), 'hour': array([3])}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "b'10.201,10.101,3,1.234,4'"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "csv_encoder = tft.coders.CsvCoder(ORDERED_SIGNATURE_COLUMNS, schema)\n",
    "records = csv_encoder.decode(\"10.201, 10.101, 3,1.234,4\")\n",
    "print(records)\n",
    "csv_encoder.encode(records)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# The Apache Beam pipeline "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_data(row):\n",
    "    print(row)\n",
    "    return row"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dry run - Everything working?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'humidity': array([23.2554], dtype=float32), 'beta1': array([3.6017134], dtype=float32), 'beta2': array([4.718712], dtype=float32), 'weekday': array([0]), 'hour': array([15])}\n",
      "{'humidity': array([24.903057], dtype=float32), 'beta1': array([1.7776823], dtype=float32), 'beta2': array([-4.6177998], dtype=float32), 'weekday': array([4]), 'hour': array([21])}\n",
      "{'humidity': array([26.00116], dtype=float32), 'beta1': array([3.5592577], dtype=float32), 'beta2': array([-2.8197079], dtype=float32), 'weekday': array([1]), 'hour': array([6])}\n",
      "{'humidity': array([24.061361], dtype=float32), 'beta1': array([3.988322], dtype=float32), 'beta2': array([4.907078], dtype=float32), 'weekday': array([1]), 'hour': array([23])}\n",
      "{'humidity': array([14.927021], dtype=float32), 'beta1': array([-2.5605125], dtype=float32), 'beta2': array([3.1700833], dtype=float32), 'weekday': array([0]), 'hour': array([2])}\n"
     ]
    }
   ],
   "source": [
    "with beam.Pipeline('DirectRunner', PipelineOptions()) as p:\n",
    "\n",
    "    csv_encoder = tft.coders.CsvCoder(ORDERED_SIGNATURE_COLUMNS, schema)    \n",
    "\n",
    "    _ = (p \n",
    "         | 'read_from_csv' >> beam.io.ReadFromText(\n",
    "             file_pattern=signature_csv_train, coder=csv_encoder, skip_header_lines=1)\n",
    "         \n",
    "         | 'process_records' >> beam.Map(process_data)\n",
    "         \n",
    "         | 'write_to_csv' >> beam.io.WriteToText(\n",
    "             file_path_prefix=training_csv, coder=csv_encoder, header=header)\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading from:  ./tmp/training.csv-00000-of-00001\n",
      "beta1,beta2,hour,humidity,weekday\n",
      "3.6017134,4.718712,15,23.2554,0\n",
      "1.7776823,-4.6177998,21,24.903057,4\n",
      "3.5592577,-2.8197079,6,26.00116,1\n",
      "3.988322,4.907078,23,24.061361,1\n",
      "-2.5605125,3.1700833,2,14.927021,0\n"
     ]
    }
   ],
   "source": [
    "!echo \"Reading from: \" $training_csv*\n",
    "!cat $training_csv* | tail -10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Serious transformation: Scale $\\beta_1$ and $\\beta_2$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "for phase_csv in [signature_csv_train, signature_csv_eval, signature_csv_test]:\n",
    "    measure(20000).to_csv(phase_csv, index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_data(row):\n",
    "    for c in ['beta1', 'beta2']:\n",
    "        row[c] = tft.scale_to_0_1(row[c])\n",
    "    return row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "signature_metadata = dataset_metadata.DatasetMetadata(schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets added to graph.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets added to graph.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:No assets to write.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:No assets to write.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:SavedModel written to: ./tmp/tftransform_tmp/5841c14891624509a576983de1444949/saved_model.pb\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:SavedModel written to: ./tmp/tftransform_tmp/5841c14891624509a576983de1444949/saved_model.pb\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets added to graph.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets added to graph.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:No assets to write.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:No assets to write.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:SavedModel written to: ./tmp/tftransform_tmp/965f1fb86e0d4e518be6d55f6aff6e5a/saved_model.pb\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:SavedModel written to: ./tmp/tftransform_tmp/965f1fb86e0d4e518be6d55f6aff6e5a/saved_model.pb\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets added to graph.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets added to graph.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:No assets to write.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:No assets to write.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:SavedModel written to: ./tmp/tftransform_tmp/6f82da2732644be889fc912f463b56e4/saved_model.pb\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:SavedModel written to: ./tmp/tftransform_tmp/6f82da2732644be889fc912f463b56e4/saved_model.pb\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n",
      "WARNING:root:Deleting 1 existing files in target path matching: -*-of-%(num_shards)05d\n"
     ]
    }
   ],
   "source": [
    "csv_encoder = tft.coders.CsvCoder(ORDERED_SIGNATURE_COLUMNS, schema)    \n",
    "tfr_encoder = tft.coders.ExampleProtoCoder(schema)            \n",
    "\n",
    "metadata_dir = os.path.join(temp_dir, \"metadata\")\n",
    "with beam.Pipeline('DirectRunner', PipelineOptions()) as p:\n",
    "\n",
    "    #\n",
    "    # The context is provided for the AnalyseAndTransform step.\n",
    "    # That step needs a hand to do its magic.\n",
    "    #\n",
    "    with tft_beam.Context(temp_dir=temp_dir):\n",
    "\n",
    "        #\n",
    "        # Read from csv, skip headers. Note that we use ordered columns in the encoder\n",
    "        # \n",
    "        signature_data = ( p | 'read_from_csv' \n",
    "            >> beam.io.ReadFromText(\n",
    "                 file_pattern=signature_csv_train, \n",
    "                coder=csv_encoder, skip_header_lines=1))\n",
    "\n",
    "        #\n",
    "        # attach the metadata: required for AnalyzeAndTransform\n",
    "        #\n",
    "        signature_data = ( signature_data, signature_metadata)\n",
    "\n",
    "        #\n",
    "        # Do the magic two steps and return also the transform-function\n",
    "        #\n",
    "        data_and_metadata, transform_fn = ( signature_data | \"AnalyzeAndTransform\" \n",
    "                         >> beam_impl.AnalyzeAndTransformDataset(process_data))\n",
    "        \n",
    "        #\n",
    "        # split data and metadata\n",
    "        #\n",
    "        training_data, training_metadata = data_and_metadata\n",
    "\n",
    "        #\n",
    "        # Write the resulting data to a csv file\n",
    "        #\n",
    "        _ = (training_data | 'write_to_csv' \n",
    "             >> beam.io.WriteToText(\n",
    "                 file_path_prefix=training_csv, coder=csv_encoder, header=header))\n",
    "\n",
    "        #\n",
    "        # For production purposes, we use the TFRecord format\n",
    "        #\n",
    "        _ = (training_data | 'write_to_tfr' \n",
    "             >> beam.io.WriteToTFRecord(\n",
    "                 file_path_prefix=training_tfr, coder=tfr_encoder))\n",
    "\n",
    "        \n",
    "        #  Process evaluation data with the obtained transform_fn\n",
    "        #\n",
    "        signature_data = ( p | 'read_from_csv_eval' \n",
    "            >> beam.io.ReadFromText(\n",
    "                 file_pattern=signature_csv_eval, coder=csv_encoder, skip_header_lines=1))\n",
    "\n",
    "        signature_data = (signature_data, signature_metadata)\n",
    "\n",
    "        # Use the transform_fn of the previous step here\n",
    "        eval_data, _ = ((signature_data, transform_fn) \n",
    "                     | \"TransformEval\" >> tft_beam.TransformDataset())\n",
    "\n",
    "        _ = (eval_data | 'write_to_tfr_eval' \n",
    "             >> beam.io.WriteToTFRecord(\n",
    "                 file_path_prefix=eval_tfr, coder=tfr_encoder))\n",
    "\n",
    "        \n",
    "        \n",
    "        #\n",
    "        # Eventually, save the transform function for re-use at prediction time.\n",
    "        #\n",
    "        _ = (transform_fn | 'WriteTransformFn' \n",
    "             >> transform_fn_io.WriteTransformFn(metadata_dir))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading from:  /tmp/tmpqmz6u8ab/training.csv-00000-of-00001\n",
      "\n",
      "0.28593916,0.44638932,12,14.948654,6\n",
      "0.08173424,0.4983325,15,10.790115,2\n",
      "0.31320256,0.51273495,2,13.007976,3\n",
      "0.07270126,0.94722366,17,9.778644,3\n",
      "0.62228477,0.73450196,3,21.455841,5\n",
      "\n",
      "metadata is here:\n",
      "transformed_metadata  transform_fn\n",
      "\n",
      "TFRecords are here:  /tmp/tmpqmz6u8ab/training.tfr-00000-of-00001\n"
     ]
    }
   ],
   "source": [
    "!echo \"Reading from: \" $training_csv*\n",
    "!echo\n",
    "!cat $training_csv* | tail -5\n",
    "!echo\n",
    "!echo metadata is here:\n",
    "!ls $metadata_dir\n",
    "!echo \n",
    "!echo \"TFRecords are here: \" $training_tfr*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wrap up"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're now able to analyze and process any size of data, scale particular features to the interval $[0, 1]$ while saving the function that actually did it for later. We'll need that function to apply exactly the same scaling to the incoming data at prediction time. \n",
    "Note that by simply swapping ```'DirectRunner'``` in the Apache Beam pipeline by ```'DataFlowRunner'``` in an adequately configured GCP environment, we could have the pipeline executed on an arbitrarily large cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
