{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.13.1'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from training_functions import make_tfr_input_fn\n",
    "import tensorflow as tf\n",
    "tf.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Take the file pattern from [Beam_Pipelines.ipynb](Beam_Pipelines.ipynb):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/tmp/tmpxhfk07ff/training.tfr-*'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from temp_dir import temp_dir\n",
    "import os\n",
    "file_pattern = os.path.join(temp_dir, \"training.tfr-*\")\n",
    "file_pattern"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we call ```train_input_fn```, we'll get a tensor that iterates through the training files and gets a new batch of records out of it each time it is being evaluated. In the end we will pass ```train_input_fn``` to the estimator, so that it can create the computational graph for the input stream within its own session and graph context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_input_fn = make_tfr_input_fn(\n",
    "    filename_pattern=file_pattern,\n",
    "    batch_size=1000, \n",
    "    options={'num_epochs': None,  # repeat infinitely\n",
    "             'shuffle_buffer_size': 1000,\n",
    "             'prefetch_buffer_size': 1000,\n",
    "             'reader_num_threads': 10,\n",
    "             'parser_num_threads': 10,\n",
    "             'sloppy_ordering': True,\n",
    "             'distribute': False})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Here, we take the tensors that provide the input stream and create an input layer from it, like described in detail in [Input_Functions.ipynb](Input_Functions.ipynb). Then we create a single ```Dense``` layer, that in essence provides the hypothesis function - a linear regression model for predicting the humidity from the the full $170$-dimensional input:\n",
    "\n",
    "$$\n",
    "h(\\beta_1, \\beta_2, \\vartheta_1, \\vartheta_2, \\dots, \\vartheta_{168} ) \n",
    "= (A_1, A_2, B_1, B_2, \\dots, B_{168}) \\cdot\n",
    "\\left( \n",
    "\\begin {array} {c}\n",
    "\\beta_1 \\\\\n",
    "\\beta_2 \\\\\n",
    "\\vartheta_1 \\\\\n",
    "\\vartheta_2 \\\\\n",
    "\\dots \\\\\n",
    "\\vartheta_{168} \n",
    "\\end{array}\n",
    "\\right) + C\n",
    "$$\n",
    "\n",
    "$\\vartheta_i$ are the components of the one-hot encoded $168$-dimensional vector for the hour-of-the-week."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/wgiersche/workspace/venv3/lib/python3.5/site-packages/tensorflow/python/feature_column/feature_column.py:205: NumericColumn._get_dense_tensor (from tensorflow.python.feature_column.feature_column_v2) is deprecated and will be removed after 2018-11-30.\n",
      "Instructions for updating:\n",
      "The old _FeatureColumn APIs are being deprecated. Please use the new FeatureColumn APIs instead.\n",
      "WARNING:tensorflow:From /home/wgiersche/workspace/venv3/lib/python3.5/site-packages/tensorflow/python/feature_column/feature_column.py:2121: NumericColumn._transform_feature (from tensorflow.python.feature_column.feature_column_v2) is deprecated and will be removed after 2018-11-30.\n",
      "Instructions for updating:\n",
      "The old _FeatureColumn APIs are being deprecated. Please use the new FeatureColumn APIs instead.\n",
      "WARNING:tensorflow:From /home/wgiersche/workspace/venv3/lib/python3.5/site-packages/tensorflow/python/feature_column/feature_column_v2.py:2703: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "WARNING:tensorflow:From /home/wgiersche/workspace/venv3/lib/python3.5/site-packages/tensorflow/python/feature_column/feature_column.py:206: NumericColumn._variable_shape (from tensorflow.python.feature_column.feature_column_v2) is deprecated and will be removed after 2018-11-30.\n",
      "Instructions for updating:\n",
      "The old _FeatureColumn APIs are being deprecated. Please use the new FeatureColumn APIs instead.\n",
      "WARNING:tensorflow:From /home/wgiersche/workspace/venv3/lib/python3.5/site-packages/tensorflow/python/feature_column/feature_column.py:205: IndicatorColumn._get_dense_tensor (from tensorflow.python.feature_column.feature_column_v2) is deprecated and will be removed after 2018-11-30.\n",
      "Instructions for updating:\n",
      "The old _FeatureColumn APIs are being deprecated. Please use the new FeatureColumn APIs instead.\n",
      "WARNING:tensorflow:From /home/wgiersche/workspace/venv3/lib/python3.5/site-packages/tensorflow/python/feature_column/feature_column.py:2121: IndicatorColumn._transform_feature (from tensorflow.python.feature_column.feature_column_v2) is deprecated and will be removed after 2018-11-30.\n",
      "Instructions for updating:\n",
      "The old _FeatureColumn APIs are being deprecated. Please use the new FeatureColumn APIs instead.\n",
      "WARNING:tensorflow:From /home/wgiersche/workspace/venv3/lib/python3.5/site-packages/tensorflow/python/feature_column/feature_column_v2.py:4295: CrossedColumn._get_sparse_tensors (from tensorflow.python.feature_column.feature_column_v2) is deprecated and will be removed after 2018-11-30.\n",
      "Instructions for updating:\n",
      "The old _FeatureColumn APIs are being deprecated. Please use the new FeatureColumn APIs instead.\n",
      "WARNING:tensorflow:From /home/wgiersche/workspace/venv3/lib/python3.5/site-packages/tensorflow/python/feature_column/feature_column.py:2121: CrossedColumn._transform_feature (from tensorflow.python.feature_column.feature_column_v2) is deprecated and will be removed after 2018-11-30.\n",
      "Instructions for updating:\n",
      "The old _FeatureColumn APIs are being deprecated. Please use the new FeatureColumn APIs instead.\n",
      "WARNING:tensorflow:From /home/wgiersche/workspace/venv3/lib/python3.5/site-packages/tensorflow/python/feature_column/feature_column_v2.py:4115: IdentityCategoricalColumn._get_sparse_tensors (from tensorflow.python.feature_column.feature_column_v2) is deprecated and will be removed after 2018-11-30.\n",
      "Instructions for updating:\n",
      "The old _FeatureColumn APIs are being deprecated. Please use the new FeatureColumn APIs instead.\n",
      "WARNING:tensorflow:From /home/wgiersche/workspace/venv3/lib/python3.5/site-packages/tensorflow/python/feature_column/feature_column.py:2121: IdentityCategoricalColumn._transform_feature (from tensorflow.python.feature_column.feature_column_v2) is deprecated and will be removed after 2018-11-30.\n",
      "Instructions for updating:\n",
      "The old _FeatureColumn APIs are being deprecated. Please use the new FeatureColumn APIs instead.\n",
      "WARNING:tensorflow:From /home/wgiersche/workspace/venv3/lib/python3.5/site-packages/tensorflow/python/feature_column/feature_column_v2.py:3828: to_int64 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "WARNING:tensorflow:From /home/wgiersche/workspace/venv3/lib/python3.5/site-packages/tensorflow/python/ops/control_flow_ops.py:3632: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From /home/wgiersche/workspace/venv3/lib/python3.5/site-packages/tensorflow/python/feature_column/feature_column_v2.py:4266: IndicatorColumn._variable_shape (from tensorflow.python.feature_column.feature_column_v2) is deprecated and will be removed after 2018-11-30.\n",
      "Instructions for updating:\n",
      "The old _FeatureColumn APIs are being deprecated. Please use the new FeatureColumn APIs instead.\n",
      "WARNING:tensorflow:From /home/wgiersche/workspace/venv3/lib/python3.5/site-packages/tensorflow/python/feature_column/feature_column_v2.py:4321: CrossedColumn._num_buckets (from tensorflow.python.feature_column.feature_column_v2) is deprecated and will be removed after 2018-11-30.\n",
      "Instructions for updating:\n",
      "The old _FeatureColumn APIs are being deprecated. Please use the new FeatureColumn APIs instead.\n"
     ]
    }
   ],
   "source": [
    "from training_functions import input_layer\n",
    "features, measured_humidity = train_input_fn()\n",
    "my_input_layer = input_layer(features)\n",
    "\n",
    "linreg = tf.layers.Dense(name=\"LinReg\", units=1)\n",
    "hypothesis=linreg(my_input_layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<tf.Tensor 'input_layer/concat:0' shape=(1000, 170) dtype=float32>,\n",
       " <tf.Tensor 'LinReg/BiasAdd:0' shape=(1000, 1) dtype=float32>)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_input_layer, hypothesis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we evaluate the hypothesis, the computational graph will now *magically* draw on batch of 1000 records from the given input file, pass it through to the hypothesis function to arrive at 1000 1-dimensional predicions - one prediction for each of the records in the current batch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "init = tf.global_variables_initializer()\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    out1 = sess.run(hypothesis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1000, 1)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out1.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "We can actually see the variables used in the ```Dense``` layer used in the hyptothesis:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "variables = hypothesis.graph.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES)\n",
    "kernel, bias = variables[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.11972031, -0.00693929,  0.0779914 , -0.18239903,  0.01535492,\n",
       "        -0.01999785, -0.03241111,  0.14207378,  0.04039174,  0.05543195,\n",
       "         0.04998854,  0.01243104,  0.11314008,  0.0405837 , -0.09234973,\n",
       "        -0.10231094, -0.07904617,  0.09446511, -0.14801048,  0.16649762,\n",
       "         0.05365515, -0.1785812 , -0.0683207 ,  0.09280813],\n",
       "       [-0.18293808,  0.11903688, -0.06728482, -0.07139375, -0.03503819,\n",
       "         0.05236694, -0.15422803,  0.09627005,  0.12292108, -0.08675345,\n",
       "         0.16064638, -0.01362239, -0.1807489 , -0.171353  , -0.00693502,\n",
       "        -0.09803757,  0.13055092,  0.020301  ,  0.10283908, -0.0361989 ,\n",
       "        -0.07657318,  0.1455619 ,  0.08403867,  0.04820745],\n",
       "       [-0.1446572 ,  0.11445534, -0.12866332,  0.1714465 , -0.12465406,\n",
       "        -0.03878172,  0.04489873,  0.11313841, -0.17282923,  0.08832672,\n",
       "        -0.16570066,  0.00542077, -0.12570232, -0.16822636,  0.11551389,\n",
       "        -0.1159942 , -0.17716137, -0.14856502,  0.05138366, -0.11191478,\n",
       "        -0.14635725,  0.01491207,  0.06730086,  0.14964381],\n",
       "       [-0.0193568 ,  0.05335343, -0.16233817,  0.03853774,  0.14598143,\n",
       "         0.03053473, -0.16177827,  0.18083441,  0.07442978, -0.05393302,\n",
       "        -0.03129028,  0.00236715,  0.14452744, -0.16656576, -0.00942543,\n",
       "        -0.18037438, -0.06000222,  0.03442442,  0.141038  , -0.02869353,\n",
       "         0.0263308 , -0.0786956 ,  0.18354946,  0.16596791],\n",
       "       [ 0.03646258,  0.07659328,  0.1042023 ,  0.01956296, -0.1805303 ,\n",
       "         0.14489022,  0.0665096 , -0.08966473,  0.16188669,  0.08180892,\n",
       "        -0.01526386,  0.10558173, -0.10320012, -0.17003459,  0.03505708,\n",
       "         0.11352611,  0.02686802,  0.17041582,  0.03700322, -0.06387311,\n",
       "        -0.13581015,  0.04791743,  0.10586417, -0.09723593],\n",
       "       [-0.07482948, -0.03419282, -0.17555678,  0.02150396, -0.16616753,\n",
       "        -0.04717939,  0.04245159, -0.10233814,  0.06647268,  0.04856648,\n",
       "        -0.00245665, -0.17295745,  0.18105376, -0.14828509,  0.09778845,\n",
       "        -0.11113471,  0.09333414,  0.1400908 ,  0.13438234,  0.14264661,\n",
       "         0.01209842,  0.14040658, -0.06594587, -0.09197982],\n",
       "       [-0.04627471, -0.14923906,  0.16564372,  0.10532147, -0.10595331,\n",
       "         0.10196   ,  0.07453659,  0.11839291, -0.05820644,  0.12627888,\n",
       "        -0.09523231, -0.0207807 , -0.15284379,  0.08778521,  0.05924273,\n",
       "        -0.13621624, -0.07212555, -0.13965756,  0.16756457, -0.14487924,\n",
       "         0.06657213, -0.00351599,  0.14005318,  0.09928948]],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    A170 = sess.run(kernel)\n",
    "A170[2:].reshape([7,24])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Now, we can manually adjust these parameters such that the predictions of the hypothesis are as close as possible to the humidity values that have actually been measured. For that, we compute the sum of the squared differences between each prediction and the measured humidity. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<tf.Tensor 'IteratorGetNext:4' shape=(1000, 1) dtype=float32>,\n",
       " <tf.Tensor 'LinReg/BiasAdd:0' shape=(1000, 1) dtype=float32>)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "measured_humidity, hypothesis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'Mean:0' shape=() dtype=float32>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss = tf.reduce_mean((hypothesis-measured_humidity)**2)\n",
    "loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One of the strengths of using an ML framework is that it usually provides means to easily compute gradients. With Tensorflow, it's as easy as the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "grad_k, grad_b = tf.gradients(loss, [kernel, bias])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<tf.Tensor 'gradients/LinReg/MatMul_grad/MatMul_1:0' shape=(170, 1) dtype=float32>,\n",
       " <tf.Tensor 'gradients/LinReg/BiasAdd_grad/BiasAddGrad:0' shape=(1,) dtype=float32>)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grad_k, grad_b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 1e-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "update_k = tf.assign_sub(kernel, grad_k * learning_rate)\n",
    "update_b = tf.assign_sub(bias, grad_b * learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In mathematical terms, what we do is:\n",
    "$$\n",
    "A \\leftarrow A - \\varepsilon \\cdot \\frac{\\partial}{\\partial A} \n",
    "L(\\vec{\\beta}, \\vec{\\vartheta},  \\vec{A}, \\vec{B}, C)\n",
    "$$\n",
    "\n",
    "$$\n",
    "B \\leftarrow B - \\varepsilon \\cdot \\frac{\\partial}{\\partial B} \n",
    "L(\\vec{\\beta}, \\vec{\\vartheta},  \\vec{A}, \\vec{B}, C)\n",
    "$$\n",
    "\n",
    "$$\n",
    "C \\leftarrow C - \\varepsilon \\cdot \\frac{\\partial}{\\partial C} \n",
    "L(\\vec{\\beta}, \\vec{\\vartheta},  \\vec{A}, \\vec{B}, C)\n",
    "$$\n",
    "\n",
    "with $A$,$B$ being the *weight* parameters for $\\beta_i$ and $\\vartheta_i$, resp., $\\varepsilon$ being the learning rate, and\n",
    "$L$ being the mean squared error ```loss``` as defined above. Computationally, evaluating ```update_k``` will have the *side effect* of changing the value of variables ```A``` and ```B``` and ```update_b``` will do that with ```C```.\n",
    "\n",
    "In short: We tweek all parameters with the help of their respective gradients."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, all we need to do is continously evaluate the ```update_b``` and ```update_k``` tensors and take note of the monotonously decreasing loss.\n",
    "\n",
    "Remember: The loss is the difference between the prediction and the *reality*, the smaller it is, the better is my hypothesis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "417.49634\n",
      "422.42154\n",
      "28.789448\n",
      "19.907553\n",
      "17.945982\n",
      "13.841923\n",
      "12.5967245\n",
      "10.655441\n",
      "9.702969\n",
      "9.201119\n",
      "8.327588\n",
      "7.7065144\n"
     ]
    }
   ],
   "source": [
    "init = tf.global_variables_initializer()\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    l = sess.run(loss)\n",
    "    print(l)\n",
    "    for i in range(101):\n",
    "        l, k, b, _, _ = sess.run([loss, kernel, bias, update_b, update_k])\n",
    "        if i % 10 == 0:\n",
    "            print(l)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using Optimizers\n",
    "Typically in Tensorflow, we don't do the gradient update ourselves. That's done by a special breed called *optimizers*. And some of them are particularly efficient in certain areas. As an example, we see the adaptive-momentum *ADAM* optimizer below. The inner workings of it are subject to ML lessions, but if you're too curious to stop here, the following link leads you to this well-known publication.\n",
    "\n",
    "[Kingma, Ba 2014 - Adam: A Method for Stochastic Optimization](https://arxiv.org/abs/1412.6980)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "features, measured_humidity = train_input_fn()\n",
    "my_input_layer = input_layer(features)\n",
    "\n",
    "linreg = tf.layers.Dense(name=\"LinReg\", units=1)\n",
    "hypothesis=linreg(my_input_layer)\n",
    "\n",
    "loss = tf.reduce_mean((hypothesis-measured_humidity)**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.train.AdamOptimizer(learning_rate=1e-0)\n",
    "train = optimizer.minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "427.7351\n",
      "3.1548233\n"
     ]
    }
   ],
   "source": [
    "init = tf.global_variables_initializer()\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    l = sess.run(loss)\n",
    "    print(l)\n",
    "    for i in range(100):\n",
    "        l, k, b, _ = sess.run([loss, kernel, bias, train])\n",
    "    print(l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
